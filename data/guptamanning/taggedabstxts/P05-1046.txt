<tag name="TECHNIQUE" value="start"/> Unsupervised Learning <tag name="TECHNIQUE" value="end"/> Of <tag name="DOMAIN" value="start"/> Field Segmentation<tag name="DOMAIN" value="end"/> Models For <tag name="DOMAIN" value="start"/>Information Extraction<tag name="DOMAIN" value="end"/> .
The applicability of many current <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> techniques is severely limited by the need for supervised training data .
We demonstrate that for certain <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>field structured extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .
Although hidden Markov models -LRB- HMMs -RRB- provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains .
However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions .
In both domains , we found that <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that <tag name="TECHNIQUE" value="start"/>semi-supervised<tag name="TECHNIQUE" value="end"/> methods can make good use of small amounts of labeled data .