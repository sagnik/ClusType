<tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Language Model Adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Incorporating <tag name="TECHNIQUE" value="start"/>Named Entity Information<tag name="TECHNIQUE" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language model -LRB- LM -RRB- adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  is important for both speech and language processing .
It is often achieved by combining a generic LM with a topic-specific model that is more relevant to the target document .
Unlike previous work on unsupervised LM adaptation , this paper investigates how effectively using <tag name="TECHNIQUE" value="start"/>named entity <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>NE<tag name="TECHNIQUE" value="end"/> -RRB- information , instead of considering all the words , helps <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>LM adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We evaluate two <tag name="TECHNIQUE" value="start"/>latent topic analysis<tag name="TECHNIQUE" value="end"/> approaches in this paper , namely , <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Latent Dirichlet Allocation<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>LDA<tag name="TECHNIQUE" value="end"/> -RRB- .
In addition , a new dynamically adapted weighting scheme for  <tag name="TECHNIQUE" value="start"/> topic mixture models<tag name="TECHNIQUE" value="end"/> is proposed based on <tag name="TECHNIQUE" value="start"/>LDA<tag name="TECHNIQUE" value="end"/> topic analysis .
Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM .
The best result is obtained using the <tag name="TECHNIQUE" value="start"/>LDA-based <tag name="TECHNIQUE" value="end"/> approach by expanding the <tag name="TECHNIQUE" value="start"/>named entities <tag name="TECHNIQUE" value="end"/> with syntactically filtered words , together with using a large number of topics , which yields a perplexity reduction of 14.23 % compared to the baseline generic LM .