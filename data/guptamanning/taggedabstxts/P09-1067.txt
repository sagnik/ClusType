<tag name="TECHNIQUE" value="start"/>Variational Decoding<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> models in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> exhibit spurious ambiguity .
That is , the probability of an output string is split among many distinct derivations -LRB- e.g. , trees or segmentations -RRB- .
In principle , the goodness of a string is measured by the total probability of its many derivations .
However , finding the best string -LRB- e.g. , during decoding -RRB- is then computationally intractable .
Therefore , most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation .
Instead , we develop a <tag name="TECHNIQUE" value="start"/>variational approximation<tag name="TECHNIQUE" value="end"/> , which considers all the derivations but still allows tractable decoding .
Our particular <tag name="TECHNIQUE" value="start"/>variational distributions<tag name="TECHNIQUE" value="end"/> are parameterized as <tag name="TECHNIQUE" value="start"/>n-gram<tag name="TECHNIQUE" value="end"/> models .
We also analytically show that interpolating thesen-gram models for different n is similar to minimumrisk decoding for BLEU -LRB- Tromble et al. , 2008 -RRB- .
Experiments show that our approach improves the state of the art .