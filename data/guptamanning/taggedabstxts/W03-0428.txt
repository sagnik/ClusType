<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Named Entity Recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> With <tag name="TECHNIQUE" value="start"/>Character-Level<tag name="TECHNIQUE" value="end"/> Models .
We discuss two <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named-entity recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models which use <tag name="TECHNIQUE" value="start"/>characters and character a4 - grams<tag name="TECHNIQUE" value="end"/> either exclusively or as an important part of their data representation .
The first model is a <tag name="TECHNIQUE" value="start"/>character-level HMM<tag name="TECHNIQUE" value="end"/> with minimal context information , and the second model is a <tag name="TECHNIQUE" value="start"/>maximum-entropy conditional markov<tag name="TECHNIQUE" value="end"/> model with substantially richer context features .
Our best model achieves an overall Fa5 of 86.07 % on the English test data -LRB- 92.31 % on the development data -RRB- .
This number represents a 25 % error reduction over the same model without word-internal -LRB- substring -RRB- features .