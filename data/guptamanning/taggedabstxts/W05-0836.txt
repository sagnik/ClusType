Training And Evaluating <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Error Minimization Decision Rules<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Decision rules<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training , often to estimate parameters in exponential models that govern the search space and the selection of candidate translations .
While the traditional Maximum A Posteriori -LRB- MAP -RRB- decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space , the Minimum Bayes Risk -LRB- MBR -RRB- decision rule is not well suited to this technique , a condition that makes past results difficult to compare .
We present a novel training approach for <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>non-tractable decision rules<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , allowing us to compare and evaluate these and other decision rules on a large scale <tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/> task , taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder .
This comparison is timely , and important , as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality .