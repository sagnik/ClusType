Enriching Automated <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Essay Scoring<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Discourse Marking<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/>Electronic Essay Rater -LRB- e-rater -RRB-<tag name="FOCUS" value="end"/> is a prototype automated <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> essay scoring<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system built at Educational Testing Service -LRB- ETS -RRB- that uses <tag name="TECHNIQUE" value="start"/>discourse marking<tag name="TECHNIQUE" value="end"/> , in addition to <tag name="TECHNIQUE" value="start"/>syntactic information and topical content vector<tag name="TECHNIQUE" value="end"/> analyses to automatically assign <tag name="DOMAIN" value="start"/>essay scores<tag name="DOMAIN" value="end"/> .
This paper gives a general description ore-rater as a whole , but its emphasis is on the importance of<tag name="TECHNIQUE" value="start"/> <tag name="FOCUS" value="start"/>discourse marking and argument partitioning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for annotating the argument structure of an essay .
We show comparisons between two <tag name="TECHNIQUE" value="start"/>content vector analysis<tag name="TECHNIQUE" value="end"/> programs used to <tag name="DOMAIN" value="start"/>predict scores<tag name="DOMAIN" value="end"/> .
EsscQ \/ ` Content and ArgContent .
EsscnContent assigns scores to essays by using a standard cosine correlation that treats the essay like a '' ` bag of words . ''
in that it does not consider word order .
<tag name="TECHNIQUE" value="start"/>Ark , Content<tag name="TECHNIQUE" value="end"/> employs a novel <tag name="TECHNIQUE" value="start"/>content vector analysis<tag name="TECHNIQUE" value="end"/> approach for score assignment based on the individual arguments in an essay .
The average agreement between ArgContent scores and human rater scores is 82 % .
as compared to 69 % agreement between EssavContent and the human raters .
These results suggest that discourse marking enriches <tag name="FOCUS" value="start"/>e-rater<tag name="FOCUS" value="end"/> 's scoring capability .
When <tag name="FOCUS" value="start"/>e-rater<tag name="FOCUS" value="end"/> uses its whole set of predictive features , agreement with human rater scores ranges from 87 Â° , \/ o - 94 % across the 15 sets of essa5 responses used in this study