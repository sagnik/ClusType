Evaluating <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Unsupervised Part-of-Speech Tagging<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammar Induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper explores the relationship between various measures of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised part-of-speech tag <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> induction and the performance of both <tag name="TECHNIQUE" value="start"/>supervised and unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> models trained on induced <tag name="TECHNIQUE" value="start"/>tags<tag name="TECHNIQUE" value="end"/> .
We find that no standard tagging metrics correlate well with <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> performance , and several metrics grounded in information theory have no strong relationship with even supervised parsing performance .