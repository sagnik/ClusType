<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Answering Clinical Questions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> with <tag name="TECHNIQUE" value="start"/>Knowledge-Based and Statistical <tag name="TECHNIQUE" value="end"/> Techniques .
The combination of recent developments in <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>question-answering<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine .
This article presents a system designed to satisfy the information needs of physicians practicing <tag name="DOMAIN" value="start"/>evidence-based medicine<tag name="DOMAIN" value="end"/> .
We have developed a series of <tag name="TECHNIQUE" value="start"/>knowledge extractors<tag name="TECHNIQUE" value="end"/> , which employ a combination of <tag name="TECHNIQUE" value="start"/>knowledge-based and statistical <tag name="TECHNIQUE" value="end"/>  techniques , for automatically identifying <tag name="DOMAIN" value="start"/> clinically relevant aspects of MEDLINE abstracts<tag name="DOMAIN" value="end"/> .
These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs , in accordance with the principles of <tag name="DOMAIN" value="start"/>evidencebased medicine<tag name="DOMAIN" value="end"/> .
Starting with an initial list of citations retrieved by PubMed , our system can bring relevant abstracts into higher ranking positions , and from these abstracts generate responses that directly answer physicians ' questions .
We describe three separate evaluations : one focused on the accuracy of the knowledge extractors , one conceptualized as a document reranking task , and finally , an evaluation of answers by two physicians .
Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline .