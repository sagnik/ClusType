Estimating the Reliability of  <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MDP Policies<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Confidence Interval<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach .
Past approaches for using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>reinforcement learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to derive <tag name="DOMAIN" value="start"/>dialog<tag name="DOMAIN" value="end"/> control policies have assumed that there was enough collected data to derive a reliable policy .
In this paper we present a methodology for numerically constructing con dence intervals for the expected cumulative reward for a learned policy .
These intervals are used to -LRB- 1 -RRB-  better assess the reliability of the expected cumulative reward , and -LRB- 2 -RRB- perform a re ned comparison between policies derived from different <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Markov Decision Processes<tag name="FOCUS" value="end"/> <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MDP<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- models .
We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace .
Our results show that while some of the policies developed in the prior work exhibited very large con dence intervals , the policy developed from the best feature set had a much smaller con dence interval and thus showed very high reliability .