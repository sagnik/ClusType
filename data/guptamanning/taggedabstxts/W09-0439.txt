Stabilizing <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Minimum Error Rate Training<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
The most commonly used method for training feature weights in <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation -LRB- SMT -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems is Och 's <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>minimum error rate training<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MERT<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- procedure .
Awell-knownproblemwithOch 's procedure is that it tends to be sensitive to small changes in the system , particularly when the number of features is large .
In this paper , we quantify the stability of Och 's procedure by supplying different random seeds to a core component of the procedure -LRB- Powell 's algorithm -RRB- .
We show that for systems with many features , there is extensive variation in outcomes , both on the development data and on the test data .
Weanalyzethecausesofthisvariationand proposemodificationstotheMERTprocedure that improve stability while helping performance on test data .