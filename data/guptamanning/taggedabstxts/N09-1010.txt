Adding More Languages Improves <tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Part-of-Speech Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : a <tag name="TECHNIQUE" value="start"/>Bayesian Non-Parametric<tag name="TECHNIQUE" value="end"/> Approach .
We investigate the problem of <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>part-of-speech tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> when raw parallel data is available in a large number of languages .
Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal .
We propose a <tag name="TECHNIQUE" value="start"/>non-parametric Bayesian<tag name="TECHNIQUE" value="end"/> model that connects related tagging decisions across languages through the use of <tag name="TECHNIQUE" value="start"/>multilingual latent variables<tag name="TECHNIQUE" value="end"/> .
Our experiments show that performance improves steadily as the number of languages increases .