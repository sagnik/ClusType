Exploiting <tag name="TECHNIQUE" value="start"/>Syntactic Structure<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language Modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The paper presents a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language model<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that develops <tag name="TECHNIQUE" value="start"/>syntactic structure<tag name="TECHNIQUE" value="end"/> and uses it to extract meaningful information from the word history , thus enabling the use of <tag name="TECHNIQUE" value="start"/>long distance dependencies<tag name="TECHNIQUE" value="end"/> .
The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner - therefore usable for <tag name="DOMAIN" value="start"/>automatic speech recognition<tag name="DOMAIN" value="end"/> .
The model , its <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> parameterization , and a set of experiments meant to evaluate its predictive power are presented ; an improvement over standard <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>trigram modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is achieved .