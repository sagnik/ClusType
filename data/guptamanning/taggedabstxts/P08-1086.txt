<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Distributed Word Clustering<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/>Large Scale Class-Based Language Modeling in <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In <tag name="FOCUS" value="start"/>statistical language modeling<tag name="FOCUS" value="end"/> , one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes .
In this paper we investigate the effects of applying such a technique to higherorder <tag name="TECHNIQUE" value="start"/> n-gram <tag name="TECHNIQUE" value="end"/>  models trained on large corpora .
We introduce a modification of the <tag name="TECHNIQUE" value="start"/>exchange clustering <tag name="TECHNIQUE" value="end"/> algorithm with improved efficiency for certain <tag name="TECHNIQUE" value="start"/>partially class-based<tag name="TECHNIQUE" value="end"/> models and a <tag name="TECHNIQUE" value="start"/>distributed<tag name="TECHNIQUE" value="end"/> version of this algorithm to efficiently obtain automatic <tag name="TECHNIQUE" value="start"/> word classifications<tag name="TECHNIQUE" value="end"/> for large vocabularies -LRB- -RRB- 1 million words -RRB- using such large training corpora -LRB- -RRB- 30 billion tokens -RRB- .
The resulting clusterings are then used in <tag name="TECHNIQUE" value="start"/> training partially class-based language <tag name="TECHNIQUE" value="end"/> models .
We show that combining them with <tag name="TECHNIQUE" value="start"/>wordbased n-gram <tag name="TECHNIQUE" value="end"/> models in the <tag name="TECHNIQUE" value="start"/>log-linear <tag name="TECHNIQUE" value="end"/> model of a state-of-the-art <tag name="DOMAIN" value="start"/>statistical machine translation<tag name="DOMAIN" value="end"/> system leads to improvements in translation quality as indicated by the BLEU score .